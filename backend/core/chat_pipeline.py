"""
å¯¹è¯å¤„ç†ç®¡é“ - ä½¿ç”¨èŒè´£é“¾æ¨¡å¼å¤„ç†å¯¹è¯è¯·æ±‚
"""

import logging
import time
import uuid
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime


logger = logging.getLogger(__name__)


@dataclass
class ChatContext:
    """å¯¹è¯ä¸Šä¸‹æ–‡ - åœ¨èŒè´£é“¾ä¸­ä¼ é€’"""
    # è¾“å…¥
    conversation_id: Optional[str] = None
    virtual_model: str = ""
    messages: List[Dict[str, Any]] = field(default_factory=list)
    user_message: str = ""
    stream: bool = False
    temperature: float = 0.7
    max_tokens: int = 2000
    
    # å¤„ç†ç»“æœ
    model_type: Optional[str] = None  # "small" | "big"
    model_config: Optional[Dict] = None
    response_content: str = ""
    final_response: Optional[Dict] = None
    
    # å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)
    request_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    start_time: float = field(default_factory=time.time)
    user_message_saved: bool = False
    error_occurred: bool = False
    skip_reason: Optional[str] = None


class PipelineHandler:
    """å¤„ç†å™¨åŸºç±»"""
    
    def __init__(self, name: str):
        self.name = name
        self._next: Optional['PipelineHandler'] = None
    
    def set_next(self, handler: 'PipelineHandler') -> 'PipelineHandler':
        """è®¾ç½®ä¸‹ä¸€ä¸ªå¤„ç†å™¨"""
        self._next = handler
        return handler
    
    async def handle(self, context: ChatContext) -> ChatContext:
        """å¤„ç†é€»è¾‘"""
        context = await self._process(context)
        
        # å¦‚æœæ²¡æœ‰è·³è¿‡åç»­å¤„ç†çš„æ ‡è®°ï¼Œç»§ç»­æ‰§è¡Œé“¾
        if self._next and not context.skip_reason and not context.error_occurred:
            return await self._next.handle(context)
        return context
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """å­ç±»å®ç°å…·ä½“é€»è¾‘"""
        raise NotImplementedError


class InputValidatorHandler(PipelineHandler):
    """è¾“å…¥éªŒè¯å¤„ç†å™¨"""
    
    def __init__(self):
        super().__init__("InputValidatorHandler")
        self.max_input_length = 10000
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """éªŒè¯è¾“å…¥"""
        # 1. å¦‚æœuser_messageä¸ºç©ºä½†æœ‰messagesï¼Œå°è¯•ä»messagesæå–
        if not context.user_message and context.messages:
            # å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            for msg in reversed(context.messages):
                if isinstance(msg, dict) and msg.get("role") == "user" and msg.get("content"):
                    context.user_message = msg.get("content", "")
                    break
        
        # 2. ç¡®ä¿user_messageæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if isinstance(context.user_message, list):
            if context.user_message:
                last_msg = context.user_message[-1]
                if isinstance(last_msg, dict):
                    context.user_message = last_msg.get("content", "")
                else:
                    context.user_message = str(last_msg)
            else:
                context.user_message = ""
        elif not isinstance(context.user_message, str):
            # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
            context.user_message = str(context.user_message) if context.user_message else ""
        
        # 3. æ£€æŸ¥ç©ºæ¶ˆæ¯
        if not context.user_message or not context.user_message.strip():
            context.error_occurred = True
            context.response_content = "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"
            logger.warning(f"[{context.request_id}] è¾“å…¥éªŒè¯å¤±è´¥: ç©ºæ¶ˆæ¯")
            return context
        
        # æ£€æŸ¥é•¿åº¦
        if len(context.user_message) > self.max_input_length:
            context.error_occurred = True
            context.response_content = f"æ¶ˆæ¯é•¿åº¦è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§{self.max_input_length}å­—ç¬¦ï¼‰"
            logger.warning(f"[{context.request_id}] è¾“å…¥éªŒè¯å¤±è´¥: è¶…é•¿è¾“å…¥")
            return context
        
        # åŸºæœ¬å®‰å…¨æ£€æŸ¥ï¼ˆç®€å•è¿‡æ»¤ï¼‰
        dangerous_patterns = [
            "<script",
            "javascript:",
            "onerror=",
            "onload=",
        ]
        
        user_input_lower = context.user_message.lower()
        for pattern in dangerous_patterns:
            if pattern in user_input_lower:
                logger.warning(f"[{context.request_id}] æ£€æµ‹åˆ°æ½œåœ¨å±é™©å†…å®¹: {pattern}")
                # åªè®°å½•æ—¥å¿—ï¼Œä¸é˜»æ­¢ï¼ˆå¯èƒ½è¯¯ä¼¤æ­£å¸¸å†…å®¹ï¼‰
                context.metadata["security_flag"] = True
                break
        
        logger.info(f"[{context.request_id}] è¾“å…¥éªŒè¯é€šè¿‡")
        return context


class UserMessagePersistence(PipelineHandler):
    """ä¿å­˜ç”¨æˆ·åŸå§‹æé—® - æœ€é«˜ä¼˜å…ˆçº§"""
    
    def __init__(self, conversation_manager):
        super().__init__("UserMessagePersistence")
        self._cm = conversation_manager
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """ä¿å­˜ç”¨æˆ·æ¶ˆæ¯"""
        try:
            # ç¡®ä¿æœ‰conversation_id
            if not context.conversation_id:
                context.conversation_id = await self._cm.create_conversation(
                    context.virtual_model
                )
                logger.info(f"[{context.request_id}] åˆ›å»ºæ–°ä¼šè¯: {context.conversation_id}")
            
            # ç«‹å³ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆä¸ç­‰å¾…åç»­å¤„ç†ï¼‰
            await self._cm.add_message(
                conversation_id=context.conversation_id,
                role="user",
                content=context.user_message,
                metadata={
                    "timestamp": datetime.utcnow().isoformat(),
                    "request_id": context.request_id,
                    "source": "webchat",
                    "ip": context.metadata.get("client_ip")
                }
            )
            
            context.user_message_saved = True
            logger.info(f"ğŸ’¾ [{context.request_id}] ç”¨æˆ·æ¶ˆæ¯å·²æŒä¹…åŒ–: {context.conversation_id}")
            
        except Exception as e:
            logger.error(f"âŒ [{context.request_id}] ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
            # ä¿å­˜å¤±è´¥ä¸ä¸­æ–­æµç¨‹ï¼Œè®°å½•é”™è¯¯ç»§ç»­
            context.metadata["user_message_save_error"] = str(e)
        
        return context


class KnowledgeRetrievalHandler(PipelineHandler):
    """çŸ¥è¯†åº“æ£€ç´¢ - å½“å‰é¢„ç•™æ¥å£ï¼Œè¯»å–é…ç½®ä½†ä¸å®ç°æ ¸å¿ƒé€»è¾‘"""
    
    def __init__(self, config_manager, skill_manager):
        super().__init__("KnowledgeRetrievalHandler")
        self._config = config_manager
        self._sm = skill_manager
    
    def _get_knowledge_config(self, virtual_model: str) -> Dict:
        """ä»config.ymlè¯»å–çŸ¥è¯†åº“é…ç½®"""
        return self._config.get(
            f"ai-gateway.virtual_models.{virtual_model}.knowledge", 
            {}
        )
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """çŸ¥è¯†æ£€ç´¢å¤„ç†ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        # è¯»å–é…ç½®
        config = self._get_knowledge_config(context.virtual_model)
        
        if not config.get("enabled", False):
            return context  # æœªå¯ç”¨ï¼Œç›´æ¥è·³è¿‡
        
        # é¢„ç•™ï¼šæ£€æŸ¥Skillé…ç½®
        skill_config = config.get("skill", {})
        if skill_config.get("enabled") and skill_config.get("version"):
            # é¢„ç•™è°ƒç”¨Skillçš„ä»£ç ï¼Œå½“å‰ä¸å®ç°
            # result = await self._sm.execute(
            #     "knowledge", f"æ£€ç´¢/{skill_config['version']}",
            #     query=context.user_message
            # )
            pass
        
        # è®°å½•å…ƒæ•°æ®
        context.metadata["knowledge_checked"] = True
        context.metadata["knowledge_enabled"] = True
        context.metadata["knowledge_skill_version"] = skill_config.get("version")
        
        logger.info(f"ğŸ“š [{context.request_id}] çŸ¥è¯†åº“æ£€ç´¢å·²é¢„ç•™ï¼ˆé…ç½®å¯ç”¨ï¼Œæš‚æœªå®ç°ï¼‰")
        return context


class WebSearchHandler(PipelineHandler):
    """è”ç½‘æœç´¢ - é¢„ç•™æ¥å£ï¼Œ4getç©ºå®ç°"""
    
    def __init__(self, config_manager, skill_manager):
        super().__init__("WebSearchHandler")
        self._config = config_manager
        self._sm = skill_manager
    
    def _get_web_search_config(self, virtual_model: str) -> Dict:
        """ä»config.ymlè¯»å–è”ç½‘æœç´¢é…ç½®"""
        return self._config.get(
            f"ai-gateway.virtual_models.{virtual_model}.web_search",
            {}
        )
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """è”ç½‘æœç´¢å¤„ç†ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        config = self._get_web_search_config(context.virtual_model)
        
        if not config.get("enabled", False):
            return context
        
        targets = config.get("target", [])
        
        # 4get ç©ºå®ç°ï¼ˆä¿ç•™ç›®å½•ï¼‰
        if "4get" in targets:
            logger.info(f"ğŸ” [{context.request_id}] 4getæœç´¢ - ç©ºå®ç°ï¼ˆç›®å½•ä¿ç•™ï¼‰")
        
        # LibreX é¢„ç•™
        if "LibreX" in targets:
            logger.info(f"ğŸ” [{context.request_id}] LibreXæœç´¢ - é¢„ç•™æ¥å£")
        
        # Skillé¢„ç•™
        skill_config = config.get("skill", {})
        if skill_config.get("enabled"):
            logger.info(f"ğŸ” [{context.request_id}] WebSearch Skillé¢„ç•™ï¼ˆç‰ˆæœ¬: {skill_config.get('version')}ï¼‰")
        
        context.metadata["web_search_checked"] = True
        context.metadata["web_search_targets"] = targets
        
        return context


class ModelRoutingHandler(PipelineHandler):
    """æ¨¡å‹è·¯ç”±å†³ç­–å¤„ç†å™¨ - åŒ…å«å…³é”®è¯åŒ¹é…å’Œæ›¿æ¢åŠŸèƒ½"""
    
    def __init__(self, model_router, config_manager=None):
        super().__init__("ModelRoutingHandler")
        self._router = model_router
        self._config = config_manager
    
    def _get_keyword_config(self, virtual_model: str) -> Dict:
        """ä»config.ymlè¯»å–å…³é”®è¯è·¯ç”±é…ç½®"""
        return self._config.get(
            f"ai-gateway.virtual_models.{virtual_model}.routing.keywords",
            {}
        ) if self._config else {}
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """æ‰§è¡Œæ¨¡å‹è·¯ç”±å†³ç­– + å…³é”®è¯æ›¿æ¢"""
        
        # 1. é¦–å…ˆå°è¯•å…³é”®è¯åŒ¹é…å’Œæ›¿æ¢
        keyword_config = self._get_keyword_config(context.virtual_model)
        
        if keyword_config.get("enabled", False):
            rules = keyword_config.get("rules", [])
            
            for rule in rules:
                pattern = rule.get("pattern", "")
                target = rule.get("target", "small")
                
                if pattern in context.user_message:
                    # åŒ¹é…æˆåŠŸï¼šåˆ‡æ¢æ¨¡å‹
                    context.model_type = target
                    context.metadata["model_type"] = target
                    context.metadata["route_reason"] = f"å…³é”®è¯åŒ¹é…: {pattern}"
                    context.metadata["matched_keyword"] = pattern
                    
                    # è®°å½•åŸå§‹æ¶ˆæ¯
                    context.metadata["original_user_message"] = context.user_message
                    
                    # ç§»é™¤å…³é”®è¯
                    remaining_message = context.user_message.replace(pattern, "").strip()
                    context.metadata["processed_user_message"] = remaining_message
                    
                    logger.info(f"ğŸ¯ [{context.request_id}] å…³é”®è¯åŒ¹é…: {pattern} -> {target}")
                    logger.info(f"ğŸ“ [{context.request_id}] æ¶ˆæ¯æ›¿æ¢: '{pattern}' -> ''")
                    logger.info(f"ğŸ“ [{context.request_id}] å‰©ä½™æ¶ˆæ¯: '{remaining_message}'")
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯å…³é”®è¯åˆ‡æ¢ï¼ˆç§»é™¤å…³é”®è¯åæ— å†…å®¹ï¼‰
                    if not remaining_message:
                        # çº¯å…³é”®è¯åˆ‡æ¢ï¼šä¸è°ƒç”¨LLMï¼Œä¸è¿”å›æ¶ˆæ¯
                        context.user_message = ""
                        context.skip_reason = "keyword_only_switch"
                        context.response_content = ""  # ç©ºå“åº”
                        logger.info(f"âœ… [{context.request_id}] çº¯å…³é”®è¯åˆ‡æ¢ï¼Œè·³è¿‡åç»­å¤„ç†")
                        return context
                    
                    # æœ‰å‰©ä½™å†…å®¹ï¼Œæ›´æ–°user_messageç»§ç»­åç»­å¤„ç†
                    context.user_message = remaining_message
                    
                    # åŒ¹é…æˆåŠŸï¼Œè·³è¿‡åç»­çš„ModelRouter.route()è°ƒç”¨
                    return context
        
        # 2. å¦‚æœæ²¡æœ‰åŒ¹é…å…³é”®è¯ï¼Œç»§ç»­åŸæœ‰ModelRouter.route()é€»è¾‘
        try:
            route_result = await self._router.route(
                virtual_model=context.virtual_model,
                user_input=context.user_message,
                conversation_id=context.conversation_id
            )
            
            context.model_type = route_result.model_type
            context.metadata["model_type"] = route_result.model_type
            context.metadata["route_reason"] = route_result.reason
            context.metadata["route_confidence"] = route_result.confidence
            context.metadata["matched_rule"] = route_result.matched_rule
            
            logger.info(f"ğŸ¯ [{context.request_id}] è·¯ç”±å†³ç­–: {route_result.model_type} - {route_result.reason}")
            
        except Exception as e:
            logger.error(f"âŒ [{context.request_id}] è·¯ç”±å†³ç­–å¤±è´¥: {e}")
            # è·¯ç”±å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
            context.model_type = "small"
            context.metadata["route_error"] = str(e)
            context.metadata["route_reason"] = "è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹"
        
        return context


class LLMInvocationHandler(PipelineHandler):
    """LLMè°ƒç”¨å¤„ç†å™¨"""
    
    def __init__(self, config_manager):
        super().__init__("LLMInvocationHandler")
        self._config = config_manager
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """è°ƒç”¨è¿œç¨‹LLM"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡ï¼ˆçº¯å…³é”®è¯åˆ‡æ¢ï¼‰
        if context.skip_reason == "keyword_only_switch":
            return context
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹
        if not context.model_type:
            logger.warning(f"[{context.request_id}] model_typeæœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤smallæ¨¡å‹")
            context.model_type = "small"
        
        try:
            # åŠ¨æ€å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
            from services.llm_service import LLMServiceFactory, ModelProvider
            from models.base import ChatCompletionRequest, Message
            
            # è·å–è™šæ‹Ÿæ¨¡å‹é…ç½®
            vm_config = self._config.get(f"ai-gateway.virtual_models.{context.virtual_model}")
            if not vm_config:
                raise ValueError(f"è™šæ‹Ÿæ¨¡å‹é…ç½®ä¸å­˜åœ¨: {context.virtual_model}")
            
            # è·å–å…·ä½“æ¨¡å‹é…ç½®
            model_config = vm_config.get(context.model_type, {})
            if not model_config:
                raise ValueError(f"æ¨¡å‹ç±»å‹é…ç½®ä¸å­˜åœ¨: {context.model_type}")
            
            # ç¡®å®šæä¾›å•†
            provider_str = model_config.get("provider", "siliconflow").lower()
            if provider_str == "openai":
                provider = ModelProvider.OPENAI
            elif provider_str == "ollama":
                provider = ModelProvider.OLLAMA
            else:
                provider = ModelProvider.SILICONFLOW
            
            # åˆ›å»ºLLMæœåŠ¡
            llm_service = LLMServiceFactory.create(
                provider=provider,
                base_url=model_config.get("base_url", "https://api.siliconflow.cn/v1"),
                api_key=model_config.get("api_key"),
                model=model_config.get("model", "Qwen/Qwen2.5-7B-Instruct"),
                temperature=context.temperature,
                max_tokens=context.max_tokens
            )
            
            # æ„å»ºè¯·æ±‚
            chat_request = ChatCompletionRequest(
                model=model_config.get("model", "unknown"),
                messages=[
                    Message(role=msg.get("role", "user"), content=msg.get("content", ""))
                    for msg in context.messages
                ],
                stream=False,
                temperature=context.temperature,
                max_tokens=context.max_tokens
            )
            
            logger.info(f"ğŸ¤– [{context.request_id}] è°ƒç”¨LLM: {provider.value}/{model_config.get('model')}")
            
            # è°ƒç”¨LLM
            response = await llm_service.chat(chat_request)
            
            # æå–å“åº”å†…å®¹
            if response.choices:
                context.response_content = response.choices[0].message.content
            
            # è®°å½•tokenä½¿ç”¨
            if response.usage:
                context.metadata["prompt_tokens"] = response.usage.prompt_tokens
                context.metadata["completion_tokens"] = response.usage.completion_tokens
                context.metadata["total_tokens"] = response.usage.total_tokens
            
            context.metadata["model_used"] = model_config.get("model")
            context.metadata["llm_called"] = True
            
            logger.info(f"âœ… [{context.request_id}] LLMè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(context.response_content)}å­—ç¬¦")
            
        except ValueError as e:
            logger.error(f"âŒ [{context.request_id}] LLMé…ç½®é”™è¯¯: {e}")
            context.error_occurred = True
            context.response_content = "æŠ±æ­‰ï¼ŒAIæœåŠ¡é…ç½®é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
            context.metadata["llm_config_error"] = str(e)
        except Exception as e:
            logger.error(f"âŒ [{context.request_id}] LLMè°ƒç”¨å¤±è´¥: {e}")
            context.error_occurred = True
            context.response_content = "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            context.metadata["llm_error"] = str(e)
        
        return context


class AssistantMessagePersistence(PipelineHandler):
    """ä¿å­˜åŠ©æ‰‹å›å¤å¤„ç†å™¨"""
    
    def __init__(self, conversation_manager):
        super().__init__("AssistantMessagePersistence")
        self._cm = conversation_manager
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """ä¿å­˜åŠ©æ‰‹å›å¤"""
        if not context.response_content or context.error_occurred:
            return context
        
        try:
            await self._cm.add_message(
                conversation_id=context.conversation_id,
                role="assistant",
                content=context.response_content,
                metadata={
                    "timestamp": datetime.utcnow().isoformat(),
                    "request_id": context.request_id,
                    "model_used": context.model_type,
                    "route_reason": context.metadata.get("route_reason"),
                    "has_knowledge": context.metadata.get("knowledge_enabled", False),
                    "has_web_search": bool(context.metadata.get("web_search_targets", []))
                }
            )
            
            logger.info(f"ğŸ’¾ [{context.request_id}] åŠ©æ‰‹å›å¤å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"âŒ [{context.request_id}] ä¿å­˜åŠ©æ‰‹å›å¤å¤±è´¥: {e}")
            context.metadata["assistant_message_save_error"] = str(e)
        
        return context


class RawDataArchiveHandler(PipelineHandler):
    """å®Œæ•´æ•°æ®å½’æ¡£å¤„ç†å™¨ - ç”¨äºè°ƒè¯•å’Œå®¡è®¡"""
    
    def __init__(self, conversation_manager):
        super().__init__("RawDataArchiveHandler")
        self._cm = conversation_manager
        self._db = conversation_manager._db
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """å½’æ¡£å®Œæ•´è¯·æ±‚/å“åº”æ•°æ®"""
        try:
            archive_data = {
                "conversation_id": context.conversation_id,
                "request_id": context.request_id,
                "timestamp": datetime.utcnow(),
                "request": {
                    "user_message": context.user_message,
                    "virtual_model": context.virtual_model,
                    "messages": context.messages,
                    "temperature": context.temperature,
                    "max_tokens": context.max_tokens,
                    "knowledge_enabled": context.metadata.get("knowledge_enabled", False),
                    "web_search_enabled": bool(context.metadata.get("web_search_targets", []))
                },
                "response": {
                    "content": context.response_content,
                    "model_type": context.model_type,
                    "model_used": context.metadata.get("model_used"),
                    "tokens_used": context.metadata.get("tokens_used"),
                    "error": context.error_occurred
                },
                "processing_metadata": context.metadata,
                "duration_ms": (time.time() - context.start_time) * 1000
            }
            
            # ä¿å­˜åˆ°raw_conversation_logsé›†åˆ
            await self._db["raw_conversation_logs"].insert_one(archive_data)
            
            logger.info(f"ğŸ“¦ [{context.request_id}] åŸå§‹æ•°æ®å·²å½’æ¡£")
            
        except Exception as e:
            logger.error(f"âŒ [{context.request_id}] å½’æ¡£åŸå§‹æ•°æ®å¤±è´¥: {e}")
            # å½’æ¡£å¤±è´¥ä¸ä¸­æ–­æµç¨‹
            context.metadata["raw_archive_error"] = str(e)
        
        return context


class ResponseFormatter(PipelineHandler):
    """å“åº”æ ¼å¼åŒ–å¤„ç†å™¨"""
    
    def __init__(self):
        super().__init__("ResponseFormatter")
    
    async def _process(self, context: ChatContext) -> ChatContext:
        """æ ¼å¼åŒ–æœ€ç»ˆå“åº”"""
        if context.error_occurred:
            context.final_response = {
                "error": {
                    "message": context.response_content or "å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯",
                    "type": "processing_error",
                    "request_id": context.request_id
                }
            }
        else:
            context.final_response = {
                "id": f"chatcmpl-{context.request_id}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": context.virtual_model,
                "conversation_id": context.conversation_id,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": context.response_content
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": context.metadata.get("prompt_tokens", 0),
                    "completion_tokens": context.metadata.get("completion_tokens", 0),
                    "total_tokens": context.metadata.get("total_tokens", 0)
                },
                "metadata": {
                    "model_type": context.model_type,
                    "route_reason": context.metadata.get("route_reason"),
                    "has_knowledge": context.metadata.get("knowledge_enabled", False),
                    "has_web_search": bool(context.metadata.get("web_search_targets", []))
                }
            }
        
        logger.info(f"ğŸ“¤ [{context.request_id}] å“åº”å·²æ ¼å¼åŒ–")
        return context


class ChatPipeline:
    """å¯¹è¯ç®¡é“ - ç»„è£…èŒè´£é“¾"""
    
    def __init__(
        self,
        conversation_manager,
        skill_manager,
        config_manager,
        model_router=None,
        llm_service=None
    ):
        self._cm = conversation_manager
        self._sm = skill_manager
        self._config = config_manager
        self._router = model_router
        self._llm_service = llm_service
        
        # æ„å»ºèŒè´£é“¾
        self._chain = self._build_chain()
    
    def _build_chain(self) -> PipelineHandler:
        """æ„å»ºå¤„ç†é“¾"""
        
        # Phase 1: è¾“å…¥å¤„ç†
        validator = InputValidatorHandler()
        user_persistence = UserMessagePersistence(self._cm)
        
        # Phase 2: é¢„å¤„ç†ï¼ˆé¢„ç•™æ¥å£ï¼‰
        knowledge = KnowledgeRetrievalHandler(self._config, self._sm)
        web_search = WebSearchHandler(self._config, self._sm)
        
        # Phase 3: æ¨¡å‹å±‚
        routing = ModelRoutingHandler(self._router, self._config)
        llm = LLMInvocationHandler(self._config)
        
        # Phase 4: åå¤„ç†
        assistant_persistence = AssistantMessagePersistence(self._cm)
        raw_archive = RawDataArchiveHandler(self._cm)
        
        # Phase 5: è¾“å‡º
        formatter = ResponseFormatter()
        
        # ç»„è£…é“¾æ¡
        validator.set_next(user_persistence) \
                 .set_next(knowledge) \
                 .set_next(web_search) \
                 .set_next(routing) \
                 .set_next(llm) \
                 .set_next(assistant_persistence) \
                 .set_next(raw_archive) \
                 .set_next(formatter)
        
        return validator
    
    async def process(self, context: ChatContext) -> ChatContext:
        """æ‰§è¡Œç®¡é“å¤„ç†"""
        return await self._chain.handle(context)


class ErrorRecoveryHandler:
    """é”™è¯¯æ¢å¤åŒ…è£…å™¨ - ç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±"""
    
    def __init__(self, conversation_manager):
        self._cm = conversation_manager
    
    async def handle_with_recovery(self, context: ChatContext, chain: PipelineHandler):
        """å¸¦é”™è¯¯æ¢å¤çš„å¤„ç†"""
        try:
            return await chain.handle(context)
        except Exception as e:
            logger.error(f"âŒ [{context.request_id}] èŒè´£é“¾æ‰§è¡Œå¤±è´¥: {e}")
            
            # ç¡®ä¿ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜ï¼ˆå¦‚æœæ²¡ä¿å­˜çš„è¯ï¼‰
            if not context.user_message_saved:
                try:
                    if not context.conversation_id:
                        context.conversation_id = await self._cm.create_conversation(
                            context.virtual_model
                        )
                    
                    await self._cm.add_message(
                        context.conversation_id,
                        "user",
                        context.user_message,
                        metadata={
                            "error_occurred": True,
                            "error_message": str(e),
                            "timestamp": datetime.utcnow().isoformat()
                        }
                    )
                    logger.info(f"ğŸ’¾ [{context.request_id}] ç´§æ€¥ä¿å­˜ç”¨æˆ·æ¶ˆæ¯æˆåŠŸ")
                except Exception as save_error:
                    logger.error(f"âŒ [{context.request_id}] ç´§æ€¥ä¿å­˜å¤±è´¥: {save_error}")
            
            # è¿”å›å‹å¥½çš„é”™è¯¯å“åº”
            context.response_content = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚"
            context.error_occurred = True
            context.metadata["error"] = str(e)
            
            return context
